{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df0R3Laf7bqy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9922-pRR8DU",
    "outputId": "4184ad86-a1a6-4024-9b04-7162089a604e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/GitKentC/dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AF8fO8M-U-o"
   },
   "outputs": [],
   "source": [
    "\"\"\" DATASET VARIABLES \"\"\"\n",
    "DATASET_NAME = 'garbage_classification'\n",
    "DATASET_DIR = f'../dataset/dupe_cleaned/{DATASET_NAME}'\n",
    "TRAIN_DIR = f'../dataset/split/{DATASET_NAME}/train'\n",
    "VALIDATION_DIR = f'../dataset/split/{DATASET_NAME}/validation'\n",
    "TEST_DIR = f'../dataset/split/{DATASET_NAME}/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" DATASET VARIABLES \"\"\"\n",
    "# # DATASET_NAME = 'garbage_classification'\n",
    "# DATASET_DIR = '/content/dataset/dupe_cleaned/garbage_classification'\n",
    "# TRAIN_DIR = '/content/dataset/split/garbage_classification/train'\n",
    "# VALIDATION_DIR = '/content/dataset/split/garbage_classification/validation'\n",
    "# TEST_DIR = '/content/dataset/split/garbage_classification/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "fH6XyAubCOCu"
   },
   "outputs": [],
   "source": [
    "\"\"\" Change this to match pre-trained models input; for consistency \"\"\"\n",
    "IMAGE_SIZE:tuple[int,int] = (256, 256) \n",
    "\n",
    "BATCH_SIZE:int = 64                     # try testing this\n",
    "LABEL_MODE:str = 'categorical'\n",
    "COLOR_MODE:str = 'rgb'                 # accepts 'grayscale', 'rgb', and 'rgba' only\n",
    "VALIDATION_SPLIT:float = 0.2           # try testing this\n",
    "SEED:int               = 42\n",
    "\n",
    "def load_datasets_cleaned():\n",
    "    \"\"\" Function to load from cleaned dataset; customizable \"\"\"\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET_DIR,\n",
    "        subset     = 'training',\n",
    "        image_size = IMAGE_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        label_mode = LABEL_MODE,\n",
    "        color_mode = COLOR_MODE,\n",
    "        validation_split = VALIDATION_SPLIT,\n",
    "        seed             = SEED,\n",
    "        pad_to_aspect_ratio=True,\n",
    "        )\n",
    "\n",
    "    validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        DATASET_DIR,\n",
    "        subset     = 'training',\n",
    "        image_size = IMAGE_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        label_mode = LABEL_MODE,\n",
    "        color_mode = COLOR_MODE,\n",
    "        validation_split = VALIDATION_SPLIT,\n",
    "        seed             = SEED,\n",
    "        pad_to_aspect_ratio=True,\n",
    "        )\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "def load_datasets_split():\n",
    "    \"\"\" Function to load from cleaned, splitted dataset  \"\"\"\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        image_size = IMAGE_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        label_mode = LABEL_MODE,\n",
    "        color_mode = COLOR_MODE,\n",
    "        pad_to_aspect_ratio=True,\n",
    "    )\n",
    "\n",
    "    validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        VALIDATION_DIR,\n",
    "        image_size = IMAGE_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        label_mode = LABEL_MODE,\n",
    "        color_mode = COLOR_MODE,\n",
    "        pad_to_aspect_ratio=True,\n",
    "    )\n",
    "\n",
    "    test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        TEST_DIR,\n",
    "        image_size = IMAGE_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        label_mode = LABEL_MODE,\n",
    "        color_mode = COLOR_MODE,\n",
    "        pad_to_aspect_ratio=True,\n",
    "    )\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQJAXXtGJOCY",
    "outputId": "c027a546-4f48-4af5-a85d-aa800c77bb38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10815 files belonging to 12 classes.\n",
      "Found 3087 files belonging to 12 classes.\n",
      "Found 1556 files belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create datasets \"\"\"\n",
    "#train_ds, val_ds = load_datasets_cleaned()\n",
    "train_ds, val_ds, test_ds = load_datasets_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train ds there is: 169 batches of 64 consisting of 10815 images!\n"
     ]
    }
   ],
   "source": [
    "print(f\"In train ds there is: {len(train_ds)} batches of {BATCH_SIZE} consisting of {len(train_ds)*BATCH_SIZE-1} images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH2sY4QTNV5q"
   },
   "outputs": [],
   "source": [
    "\"\"\" Preprocess the data  \"\"\"\n",
    "def preprocess(image, label):\n",
    "    \"\"\" Read the docs in https://github.com/keras-team/keras/tree/master/keras/src/applications for more info \"\"\"\n",
    "    # CenterCrop after resizing the smallest\n",
    "    image = tf.keras.layers.CenterCrop(224,224)(image)\n",
    "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds_prep = train_ds.map(preprocess)\n",
    "val_ds_prep = val_ds.map(preprocess)\n",
    "test_ds_prep = test_ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_prep.element_spec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    original_image = images[0].numpy().astype(\"uint8\")  \n",
    "    label = labels[0].numpy()\n",
    "\n",
    "    preprocessed_image = preprocess(original_image, label)  \n",
    "\n",
    "    # Display both images side by side\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "    plt.imshow(original_image)\n",
    "    plt.title(f\"Original Image\\nLabel: {label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Preprocessed image\n",
    "    plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "    plt.imshow(preprocessed_image[0])\n",
    "    plt.title(\"Preprocessed Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uy85BY1mJVSM"
   },
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds_final = (train_ds_prep\n",
    "                  .cache()\n",
    "                  .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                  .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                  )\n",
    "\n",
    "val_ds_final = (val_ds_prep\n",
    "                .cache()\n",
    "                .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                )\n",
    "\n",
    "test_ds_final = (test_ds_prep\n",
    "                .cache()\n",
    "                .prefetch(PREFETCH_BUFFER_SIZE)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KKIsyi8cmEoK",
    "outputId": "080f1319-3c0a-4936-cd76-6cf1091e6af0"
   },
   "outputs": [],
   "source": [
    "def create_pre_trained_model():\n",
    "    pre_trained_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        input_shape=(224,224,3),\n",
    "        weights=\"imagenet\"\n",
    "    )\n",
    "\n",
    "    for layer in pre_trained_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return pre_trained_model\n",
    "\n",
    "pre_trained_model = create_pre_trained_model()\n",
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzC5jecQ7XX3",
    "outputId": "308fc715-b4c7-4de6-9b11-aff2c2dfc0d7"
   },
   "outputs": [],
   "source": [
    "last_output = pre_trained_model.output\n",
    "\n",
    "print(last_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentation_model():\n",
    "    FILL_MODE = 'nearest'\n",
    "\n",
    "    augmentation_model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(224, 224, 3)),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.2, fill_mode = FILL_MODE),\n",
    "        tf.keras.layers.RandomTranslation(0.2, 0.2, fill_mode = FILL_MODE),\n",
    "        tf.keras.layers.RandomZoom(0.2, fill_mode = FILL_MODE),\n",
    "    ])\n",
    "\n",
    "    return augmentation_model\n",
    "\n",
    "augmentation_model = create_augmentation_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cZExu2QUMHB"
   },
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = augmentation_model(inputs)\n",
    "\n",
    "    x = pre_trained_model(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(hp.Int('dense_units', min_value=32, max_value=2048, step=2, sampling='log'), \n",
    "                              activation='relu', \n",
    "                              kernel_regularizer=regularizers.l2(hp.Float('dense_l2', min_value=1e-5, max_value=1e-2))\n",
    "                              )(x)\n",
    "    x = tf.keras.layers.Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.66))(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(12, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=3e-4)),\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "11aY2gSSXcHQ",
    "outputId": "75deab2e-fed3-4463-c855-43b23ec432fb"
   },
   "outputs": [],
   "source": [
    "summary = create_model(kt.HyperParameters())\n",
    "summary.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=create_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=50,\n",
    "    num_initial_points=5,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory='hypertune',\n",
    "    project_name='resnet50_hypertune_trial1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hAde-xPc76t"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the log directory\n",
    "log_dir = 'logs/hyperparameter_tuning'\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',       # Monitors the validation loss\n",
    "    min_delta=0.001,          # Minimum change to qualify as an improvement\n",
    "    patience=5,               # Number of epochs to wait for improvement\n",
    "    verbose=1,                # Verbosity mode\n",
    "    mode='min',               # Stop when the monitored quantity has stopped decreasing\n",
    ")\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tuner.search(\n",
    "    train_ds_final,\n",
    "    validation_data=val_ds_final,\n",
    "    epochs=50,\n",
    "    callbacks=[tensorboard_callback, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xkn8c39XhzH",
    "outputId": "dc20e368-9a09-4015-f175-5c223fc866c8"
   },
   "outputs": [],
   "source": [
    "# history = model.fit(train_ds_final,\n",
    "#                     validation_data=val_ds_final,\n",
    "#                     epochs=30,\n",
    "#                     verbose=1,\n",
    "#                     callbacks = [early_stopping],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3p1j3Rh3g9b"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "CQShbbLLOMnU",
    "outputId": "df945a9f-0611-457b-810e-886b8d9ca3f5"
   },
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    '''Plots the training and validation loss and accuracy from a history object'''\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "    ax[0].plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    ax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    ax[0].set_title('Training and validation accuracy')\n",
    "    ax[0].set_xlabel('epochs')\n",
    "    ax[0].set_ylabel('accuracy')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(epochs, loss, 'bo', label='Training Loss')\n",
    "    ax[1].plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "    ax[1].set_title('Training and validation loss')\n",
    "    ax[1].set_xlabel('epochs')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYxEyyug3iJs",
    "outputId": "140092bf-d793-4f77-f43d-b547087a1b05"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds_final, verbose=1)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HV2tkGpB_-NL",
    "outputId": "56c6b2f1-b067-4546-dda3-f7dfbd4613e4"
   },
   "outputs": [],
   "source": [
    "model_name = \"resnet50_finetune_trial1.h5\"\n",
    "tf.keras.models.save_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('resnet50_finetune_trial1.h5')\n",
    "\n",
    "loaded_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "\n",
    "# (Optional) Optimize the model for better performance and smaller size\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model converted to TensorFlow Lite format and saved as model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
